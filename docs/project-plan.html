<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
     <link rel="StyleSheet" type="text/css"
	   href="./styles/default.css" title="Main style" />
     <title>Ovirt Project Plan</title>

  </head>
  <body>
    <div id="header">
      <h1>Ovirt Project Plan</h1>
    </div>
    <div id="content">
    <h2>Overview</h2>
    <h3>Ovirt is:</h3>
    <ol>
      <li>A tiny hypervisor/dom0 kernel that installs and (optionally)
   runs off a CD, a usb key, or even a ramdisk (over PXE) on bare
   metal. At a minimum includes libvirtd for management, meaning the
   box and its guests would be manageable by virt-manager for small
   numbers of machines.</li>

      <li>A web-based management UI appliance for the tiny
   hypervisor/domUs as well as the guests running on them. Rather than
   the traditional machine -> guests -> resources paradigm, the admin
   UI will be focused on grouping resources (cpu, memory, disk,
   network, redundancy) into pools, granting quota on those pools to
   users, then allowing each user to manage VMs within his quota as
   needed via a separate user UI.</li>
      
      <li>A pre-configured freeIPA appliance that will provide
	the Kerberos infrastructure as well as the
	authorization/authentication tools for the above.</li>

      <li>All of the above install to defaults with zero config input, but
	are customizable for large-scale installations or edge cases.</li>
    </ol>

    <h2>Status at-a-glance</h2>
    <ul>
      <li>Milestone 1 complete on-time. Minimal dom0 with KVM installs
      to a ramdisk from either pxe or a usb key (your
      choice). Restrictions: requires fedora 8, x86_64; requires hacks
      to virtinst to install a guest remotely (libvirt storage API not
	yet available).</li>
      <li>Milestone 2 finished, one week late (December
      7). Installation is still very cumbersome, but we have 
      integration with FreeIPA for authentication and we can create, stop, start,
      suspend, and resume guests from the UI. In addition we can
	create and edit user quotas and we automatically enforce them.</li>
      <li>Milestone 2.5, in process now and due by year-end, focuses
      on making what we have easy to deploy, and on doing required schema
      changes to implement the resource pool/quota/permission
	structure we have added to the project.</li>

      <li>Milestone 3 in planning stage, deadline moved to end of
      January. Milestone 3 aims to have a publicly releasable project
      usable by developers.</li>
    </ul>

    <h2>Plan</h2>
    <h3>Milestone 1 (First week Nov)</h3>

    <p>Goal: Bootstrap initial development process</p>

    <h3>Docs:</h3>

    <p>A quick-and-dirty developer's setup guide to the following:</p>
    <ul>
      <li>Prepare image for pxe</li>
      <li>Install iSCSI target locally and create LUNs (no, not
	an iSCSI manual, just the necessary quick tips)</li> 
      <li>Set up cobbler or manually do DHCP config</li>
      <li>Install minimal host</li>
      <li>Install guest on minimal host</li>
    </ul>

    <h3>Host:</h3>

    <ul>
      <li>PXE boot host image; Run from RAM; No permanent state
	<ul>
	  <li>Have a first crack at specifying and building a slimmed-down
	    host image</li>
	  <li>Automate the process so that as the component bits change anyone
	    can go get a current version and install it</li>
	  <li>Provide a PXE kernel/initrd and a ks that will download the
	    rest of the image over tftp and install it to a RAM disk</li>
	  <li>Optionally: provide a canned Cobbler distro/profile for
	    auto-configuring the dhcp/tftp for the image.</li>
	</ul>
      </li>
  
      <li>NICs bridged to LAN & use DHCP config only
	<ul>
	  <li> Create temporary init scripts to put host nic into
      bridging mode for guests. Later we'll extend libvirt networking
	    to handle this better.</li>
	</ul>
      </li>
      <li>iSCSI storage; LUNs pre-allocated on iSCSI server; no local
	partitioning
	<ul>
	  <li>iSCSI server address is predefined in DHCP option records</li>
	  <li>libvirt able to query local iSCSI initiator to list LUNs and
	    mapped paths (this is optional but helpful for milestone 1)</li>
	  <li>Be sure iSCSI target/initiator actually work for our purposes</li>
	</ul>
      </li>
      <li>All per-host config data from DHCP option records or hardcoded in
	image</li>

      <li>Fetch pre-generated kerberos credentials from HTTP server.</li>
    
      <li>Sanity check virtinst to see that it will successfully install a
      remote full-virt guest via PXE (i.e. without pulling down a
      kernel/initrd or trying to create a disk). Ideally, make it use
	libvirt LUN enumeration to check disk path exists.</li>
    </ul>


    <h3>Management:</h3>

    <ul>
      <li>Guest install & admin with virt-manager</li>
      <li>Web interaction design</li>
    </ul>

    <h2>Milestone 2  (Last week Nov)</h2>

    <p>Goal: Network service integration, functional management UI</p>

    <h3>Host:</h3>

    <ul>
      <li>Persist configuration data (USB stick/flash/disk)<br />
    The starting point for this should be making /etc/libvirt an nfs
    mount I think. If we're using a USB stick for config persistence
	later we can mount it in the same location.</li>

      <li>Manual NIC address configuration<br />
	Use PXE/DHCP to configure the primary NIC on the host. The
	remainder (assuming there's more than one) will be
	auto-configured to some sensible default, but not until
	milestone 3.</li>


      <li>Manual iSCSI configuration; iSCSI authentication<br />
	iSCSI config can passed in in DHCP option records at the
	moment. We may do this with LDAP/FreeIPA later.</li>

      <li>FreeIPA integration for credential deployment<br />
	For now we are manually distributing keytabs and kerberos conf
	to hosts as they install. FreeIPA has a tool coming  that will
	take over this part and do it automatically.</li> 

      <li>Local disk for swap<br />
	Automatically find local disk where it exists and allocate a
	suitable swap partition. 
      </li>
    </ul>

    <h3>Management:</h3>

    <ul>
      <li>Host &lt;-&gt; web service association<br /> When a host comes up
	it will automatically advertise its presence via Avahi. On
	the manager side, we need a worker to respond to that,
	identify the host (via FreeIPA), and present it in the
	manager UI.</li>

      <li>PXE boot guest OS installation<br />
	Be able to tell a host to start a guest with a particular MAC
	address that will PXE off of the management app, assuming we
	have control of the PXE server.</li> 

      <li>Web service authentication against FreeIPA<br />
	We will need the manager app to check that the user has
	a current Kerberos principle and that they exist as a manager
	app user, and if not redirect to some appropriate place.
      </li>

      <li>Allocate resources to users<br />
	We need a way to model users, physical resources
	(cpu/disk/memory/bandwidth), and quotas. We also need some notion
	of how to enforce a quota. We also need a notion of the
	maximum resources a guest can use on a particular machine,
	regardless of the user's quota size: e.g. you can't assign
	more memory than is available on a physical machine to a
	guest, even if the user has more memory than that in their
	quota.
      </li>

      <li>Live guest performance monitoring<br />
	Collectd will let us do some of this but not until we have
	some improvements in KVM that are coming for Milestone 3.
      </li>

      <li>Live host performance monitoring<br />
	We will have collectd in the install image for this milestone
	and we already have a libvirt plug-in for it, so we'll be able
	to monitor host stats from a console on the manager at
	least. No pretty graphs for this milestone though.</li>
    </ul>
  
    <h2>Milestone 2.5 (December 21)</h2>
    <p>Goal: Packaging (developers need to be able to download and
      install host image easily)</p>
    <p>NB: This milestone is still for developers only. Installation
    will be easier but we do not expect guest storage to be
      configurable from the UI by this release.</p>

    <h3>Docs:</h3>
    <ul>
      <li>We need a proper name for the project and a public
      website. On the website we need a yum repo, an up-to-date
	install doc, a way to download source tars, and a user
      doc.</li>
      <li>Update install docs</li>
      <li>Create user manual doc</li>
    </ul>

    <h3>Host:</h3>

    <ul>
      <li>Prebuilt host images for pxe, flash drive, and CDROM
      available for download.<br />
	Having the image prebuilt means the PXE initrd needs to
      autodetect the network card so that it can download the full
      host image. Also we will try to slim the image down as much as
      we can without getting into breaking up packages (that will come
      later). Finally we need to automaticall obtain krb5.conf and
      keytab for each image we're going to install and stuff those two
      files into the image. This should mean that a prebuilt image can
      be customized for an installation with one command, and then
      booted on a host.
      </li>
    </ul>
    <h3>Manager:</h3>
    <ul>

      <li>Update the schema to reflect the changes from our planning
	meetings of December 10-11. These are
	detailed <a href="ovirt-plan-update.txt">here</a>; in summary
	we are abandoning the admin/user divide from the first
	prototype and instead establishing a way to group machines and
	delegate control over groups of machines to certain users.
      </li>
      <li>Update the existing UI views to work with the new schema</li>
      <li>Implement the first cut of the new permissions system. No
      integration with LDAP yet and no groups, but check user
	permissions before allowing views/actions.</li>
      <li>Fix the taskomatic bit so that it runs as a service user
      with its own kerberos credential, so that the user doesn't have
	to kinit as root to get the wui to run.</li>
    </ul>
    
    <h2>Milestone 3 (Late January -- Fedora Test 1 timeframe)</h2>

    <p>Goal: End to end demo of all basic features (and
      unveiling). Demo includes:</p>
    <ul>
      <li>Create a resource pool with an ovirt host and a storage
      provider</li>
      <li>Add 2 users to the pool from the available users in FreeIPA</li>
      <li>Give each user quota on the pool</li>
      <li>Log in as each user and create a guest, assigning it storage
	from the pool</li>
      <li>Start the guests and demonstrate that they install
      successfully, choosing the install image from a list of
      available isos in the system.</li>
      <li>Access the guests using virt-viewer</li>
      <li>Stop, start, suspend, resume, save, restore the guests</li>
      <li>Demonstrate that users can't exceed quota</li>
      <li>Demonstrate that one user can't access another's guests</li>
      <li>Demonstrate that state updates both on the guest view and
	on the host view</li>
    </ul>

    <h3>Docs:</h3>

    <ul>
      <li>Update install doc</li>
      <li>Create tutorial doc for user manual</li>
    </ul>

    <h3>Host:</h3>
    
    <ul>
      <li>Finish last bits of host install -- including automatically
	rolling in the kerberos config and keytab for the image.</li>
      <li>Build guest with libvirt w/storage API patches (if they're
	not already in a released version)</li>
      <li>Add hooks to collectd to feed us info on guest state (for
      updating status)</li>
    </ul>

    <h3>Manager:</h3>
    
    <ul>
      <li>Make permission system work and integrate with
      LDAP/FreeIPA</li>
      <li>Make necessary alterations to former admin pages so they
	work as resource pool pages.</li>
      <li>Add user management for resource pools</li>
      <li>Update quota UI to reflect new quota attributes: CPUs, RAM,
      storage, NICs, VMs</li>
      <li>UI scaling, stage 1: Make sure any widget with a list that
	may get large is limited and paginated (stage 2 adds
	filtering)</li>
      <li>Add management for available install ISOs to resource pool
      pages</li>
      <li>Add ISO chooser to VM install page</li>
      <li>Do NSAPI plug-in for virt-viewer on linux, add it to VM view
      page</li>
      <li>Add auto-refresh for VM and host status widgets so that
	state information is current</li>
      <li>Make WUI install seamless (rpm, &quot;firstboot&quot; script
	run from init script)</li>
      <li>Storage API integration, as API becomes available. This
      means UI for enumerating LUNs on an iSCSI target, UI for
      enumerating LVs in a volume group, UI for enumerating
      partitions on a disk; and finally UI for choosing one of these
      for a guest.</li>
      <li>Add UI to resource pool for manual VM placement and resource
      limitation. In other words, we need a resource pool owner to be
      able to control placement and migration of any VMs running in
      their pool, and also we need to be able to dynamically CPU-limit
      VMs and groups of VMs running in a pool. This is a precursor to
	automating this kind of management down the line.</li>
    </ul>

    <h2>Milestone 4, AKA Release 1 (March 4, F9t2 timeframe)</h2>

    <p>Goal: Clustering and NIC bonding</p>

    <h3>Host:</h3>

    <ul>

      <li>Partitioning of iSCSI LUNs on host; LVM management of iSCSI
    LUNs for guest image. </li>
      <li>NIC bonding for failover and bandwidth aggregation<br />
	Have a reasonable set of default configs on a baremetal
	install depending on the number of available NICs. The obvious
	place to start is 1 NIC for storage, 1 for management, and 1
	per guest. Be able to modify this from the management app so
	that we can for example bond NICs for storage or for guest
	use.</li>
      <li>Manage host pairing with quorum disk</li>
      <li>Cluster-LVM awareness</li>

    </ul>

    <h3>Management:</h3>

    <ul>
      <li>Delegation of host administration to owners<br />
	Users should be able to manage
	resources within their assigned quota -- e.g. if you have 2
	physical NICs available decide if you want them bonded or if you
	want each dedicated to a different guest, decide how you want
	your storage dealt with, etc.</li>    

      <li>Long term performance monitoring<br /> 
      With collectd in
      place on installed hosts, be able to store snapshots of host and guest
      performmance over time and display them graphically. With some
      additional work in KVM, collectd will let us collect per-guest
	stats for network i/o, disk i/o, and CPU usage.</li>

      <li>Grouping of hosts for clusters</li>

      <li>Deployment of guests to cluster</li>
      
      <li>Expand manager UI<br />
	Add UI for network management, storage management, automation
	(auto-failover e.g.). Continue to polish existing user and
	admin UI.
      </li>
    </ul>

    <h2>Future work (no particlar order</h2>
    
    <h3>Host:</h3>

    <ul>
      <li>NFS/GFS/Fibrechannel storage options (no longer restricted
	to iSCSI)</li>
      <li>Access guest consoles directly on host hardware<br />
	The beginnings of the local host &quot;bios&quot; for local
	machine management.
      </li>
      <li>Manage internal IDE/SCSI storage, USB disks, firewire
      disks<br />
	Libvirt storage API will handle this for us, we just need
	manager UI to cope with it.</li>
      <li>Install guest from local CDROM</li>
      <li>Pre-emptive live migration, post failure offline
	migration.<br />  Have a monitoring or stats daemon running on
	the host that will generate events we can respond to from the
	management app. Responses would take the form of migrating a
	running guest automatically, or possibly offline migrating a
	stopped guest. collectd may be helpful with this.</li>
    </ul>
    <h3>Management</h3>
    
    <ul>
      <li>Management appliance<br />
	Be able to deploy the management app as an
    appliance. </li>

      <li>Automatic migration upon failure.<br />
	This should be the beginning of a policy engine that would
	eventually include dynamic scheduler changes, memory
	allocation, and so on.</li>

      <li>FreeIPA appliance, Cobbler appliance, iSCSI server
	appliance</li>
      <li>Additional UI from milestone 4 polished up.</li>
      <li>Testing on various host configurations</li>
    </ul>

    </div>
    <div id="footer">
       <p>
	 Web design courtesy Mairin Duffy and Dan Berrange<br />
    A project from the <a href="http://et.redhat.com/">Red Hat Emerging Technologies</a> group.
    </p>
  </body>
</html>
